执行批量加载命令...
/usr/bin/which: no hbase in (/opt/onlineedu/hadoop/bin:/opt/onlineedu/rabbitmq/sbin:/opt/onlineedu/otp/erlang/bin:/opt/onlineedu/sqoop/bin:/opt/onlineedu/kafka/bin:/opt/onlineedu/hive/bin:/opt/onlineedu/spark/bin:/opt/onlineedu/scala/bin:/opt/onlineedu/hadoop/bin:/opt/onlineedu/hadoop/sbin:/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.312.b07-2.el8_5.x86_64//bin:/home/niit/.local/bin:/home/niit/bin:/opt/onlineedu/rabbitmq/sbin:/opt/onlineedu/otp/erlang/bin:/opt/onlineedu/sqoop/bin:/opt/onlineedu/kafka/bin:/opt/onlineedu/hive/bin:/opt/onlineedu/spark/bin:/opt/onlineedu/scala/bin:/opt/onlineedu/hadoop/bin:/opt/onlineedu/hadoop/sbin:/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.312.b07-2.el8_5.x86_64//bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin)
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/onlineedu/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/onlineedu/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

Logging initialized using configuration in jar:file:/opt/onlineedu/hive/lib/hive-common-2.3.7.jar!/hive-log4j2.properties Async: true
Loading data to table bank_db.ods_modify_logs_inc partition (dt=2024-12-18)
OK
Time taken: 3.882 seconds
Loading data to table bank_db.ods_modify_logs_inc partition (dt=2024-12-18)
OK
Time taken: 0.447 seconds
Loading data to table bank_db.ods_modify_logs_inc partition (dt=2024-12-18)
OK
Time taken: 0.344 seconds
所有文件加载成功，日期: 2024-12-18
/usr/bin/which: no hbase in (/opt/onlineedu/rabbitmq/sbin:/opt/onlineedu/otp/erlang/bin:/opt/onlineedu/sqoop/bin:/opt/onlineedu/kafka/bin:/opt/onlineedu/hive/bin:/opt/onlineedu/spark/bin:/opt/onlineedu/scala/bin:/opt/onlineedu/hadoop/bin:/opt/onlineedu/hadoop/sbin:/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.312.b07-2.el8_5.x86_64//bin:/home/niit/.local/bin:/home/niit/bin:/opt/onlineedu/rabbitmq/sbin:/opt/onlineedu/otp/erlang/bin:/opt/onlineedu/sqoop/bin:/opt/onlineedu/kafka/bin:/opt/onlineedu/hive/bin:/opt/onlineedu/spark/bin:/opt/onlineedu/scala/bin:/opt/onlineedu/hadoop/bin:/opt/onlineedu/hadoop/sbin:/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.312.b07-2.el8_5.x86_64//bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin)
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/onlineedu/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/onlineedu/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

Logging initialized using configuration in jar:file:/opt/onlineedu/hive/lib/hive-common-2.3.7.jar!/hive-log4j2.properties Async: true
Warning: Map Join MAPJOIN[41][bigTable=?] in task 'Stage-1:MAPRED' is a cross product
Query ID = niit_20241218173719_4c5459ce-8010-47ac-8c79-956ebc0e70e2
Total jobs = 2
Launching Job 1 out of 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Spark Job = be64695b-5cda-4e71-9cb7-9a8ac5ff0ca4
Running with YARN Application = application_1732546889003_0067
Kill Command = /opt/onlineedu/hadoop/bin/yarn application -kill application_1732546889003_0067

Query Hive on Spark job[0] stages: [0, 1, 2]

Status: Running (Hive on Spark job[0])
Job Progress Format
CurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount
2024-12-18 17:37:40,783	Stage-0_0: 0/1	Stage-1_0: 0/1	Stage-2_0: 0/2	
2024-12-18 17:37:42,799	Stage-0_0: 1/1 Finished	Stage-1_0: 0(+1)/1	Stage-2_0: 0/2	
2024-12-18 17:37:43,805	Stage-0_0: 1/1 Finished	Stage-1_0: 1/1 Finished	Stage-2_0: 2/2 Finished	
Status: Finished successfully in 6.06 seconds
Launching Job 2 out of 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Spark Job = b05efc44-370d-4553-a748-ff5a1c6d6c03
Running with YARN Application = application_1732546889003_0067
Kill Command = /opt/onlineedu/hadoop/bin/yarn application -kill application_1732546889003_0067

Query Hive on Spark job[1] stages: [3, 4]

Status: Running (Hive on Spark job[1])
Job Progress Format
CurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount
2024-12-18 17:37:45,867	Stage-3_0: 0/3	Stage-4_0: 0/2	
2024-12-18 17:37:46,909	Stage-3_0: 1(+1)/3	Stage-4_0: 0/2	
2024-12-18 17:37:48,963	Stage-3_0: 3/3 Finished	Stage-4_0: 0/2	
2024-12-18 17:37:50,973	Stage-3_0: 3/3 Finished	Stage-4_0: 0(+1)/2	
2024-12-18 17:37:51,978	Stage-3_0: 3/3 Finished	Stage-4_0: 2/2 Finished	
Status: Finished successfully in 8.12 seconds
Loading data to table bank_db.dwd_customer_data_full
OK
final_data.customerid	final_data.surname	final_data.creditscore	final_data.geography	final_data.gender	final_data.age	final_data.tenure	final_data.balance	final_data.numofproducts	final_data.hascrcard	final_data.isactivemember	final_data.estimatedsalary	final_data.exited
Time taken: 32.625 seconds
ODS 数据成功转换并加载到 DWD 层，日期: 2024-12-18
/usr/bin/which: no hbase in (/opt/onlineedu/rabbitmq/sbin:/opt/onlineedu/otp/erlang/bin:/opt/onlineedu/sqoop/bin:/opt/onlineedu/kafka/bin:/opt/onlineedu/hive/bin:/opt/onlineedu/spark/bin:/opt/onlineedu/scala/bin:/opt/onlineedu/hadoop/bin:/opt/onlineedu/hadoop/sbin:/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.312.b07-2.el8_5.x86_64//bin:/home/niit/.local/bin:/home/niit/bin:/opt/onlineedu/rabbitmq/sbin:/opt/onlineedu/otp/erlang/bin:/opt/onlineedu/sqoop/bin:/opt/onlineedu/kafka/bin:/opt/onlineedu/hive/bin:/opt/onlineedu/spark/bin:/opt/onlineedu/scala/bin:/opt/onlineedu/hadoop/bin:/opt/onlineedu/hadoop/sbin:/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.312.b07-2.el8_5.x86_64//bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin)
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/onlineedu/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/onlineedu/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

Logging initialized using configuration in jar:file:/opt/onlineedu/hive/lib/hive-common-2.3.7.jar!/hive-log4j2.properties Async: true
Query ID = niit_20241218173756_dcf3de7f-1cf5-4e03-92dd-5b706d8f447d
Total jobs = 1
Launching Job 1 out of 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Spark Job = e3a7ad3d-a702-4de7-abae-192e42e1283d
Running with YARN Application = application_1732546889003_0068
Kill Command = /opt/onlineedu/hadoop/bin/yarn application -kill application_1732546889003_0068

Query Hive on Spark job[0] stages: [0, 1, 2]

Status: Running (Hive on Spark job[0])
Job Progress Format
CurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount
2024-12-18 17:38:14,176	Stage-0_0: 0(+1)/3	Stage-1_0: 0/16	Stage-2_0: 0/16	
2024-12-18 17:38:17,334	Stage-0_0: 1(+1)/3	Stage-1_0: 0/16	Stage-2_0: 0/16	
2024-12-18 17:38:18,379	Stage-0_0: 3/3 Finished	Stage-1_0: 0/16	Stage-2_0: 0/16	
2024-12-18 17:38:19,447	Stage-0_0: 3/3 Finished	Stage-1_0: 5(+3)/16	Stage-2_0: 0/16	
2024-12-18 17:38:20,485	Stage-0_0: 3/3 Finished	Stage-1_0: 16/16 Finished	Stage-2_0: 0/16	
2024-12-18 17:38:21,510	Stage-0_0: 3/3 Finished	Stage-1_0: 16/16 Finished	Stage-2_0: 1(+3)/16	
2024-12-18 17:38:22,551	Stage-0_0: 3/3 Finished	Stage-1_0: 16/16 Finished	Stage-2_0: 8(+1)/16	
2024-12-18 17:38:23,603	Stage-0_0: 3/3 Finished	Stage-1_0: 16/16 Finished	Stage-2_0: 10(+2)/16	
2024-12-18 17:38:24,619	Stage-0_0: 3/3 Finished	Stage-1_0: 16/16 Finished	Stage-2_0: 14(+2)/16	
2024-12-18 17:38:25,624	Stage-0_0: 3/3 Finished	Stage-1_0: 16/16 Finished	Stage-2_0: 16/16 Finished	
Status: Finished successfully in 13.54 seconds
Loading data to table bank_db.dim_customer_lifecycle_zip
OK
customerid	surname	creditscore	geography	gender	age	tenure	balance	numofproducts	hascrcard	isactivemember	estimatedsalary	exited	effective_date	expiration_date	is_current
Time taken: 29.774 seconds
Query ID = niit_20241218173826_f34bb8f6-2743-4437-8b15-74f250106781
Total jobs = 1
Launching Job 1 out of 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Spark Job = 180fa51c-67ae-4931-b67e-30358b0f309f
Running with YARN Application = application_1732546889003_0068
Kill Command = /opt/onlineedu/hadoop/bin/yarn application -kill application_1732546889003_0068

Query Hive on Spark job[1] stages: [3, 4]

Status: Running (Hive on Spark job[1])
Job Progress Format
CurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount
2024-12-18 17:38:27,577	Stage-3_0: 1/1 Finished	Stage-4_0: 0/16	
2024-12-18 17:38:28,582	Stage-3_0: 1/1 Finished	Stage-4_0: 2(+2)/16	
2024-12-18 17:38:29,607	Stage-3_0: 1/1 Finished	Stage-4_0: 13(+2)/16	
2024-12-18 17:38:30,611	Stage-3_0: 1/1 Finished	Stage-4_0: 16/16 Finished	
Status: Finished successfully in 4.05 seconds
Loading data to table bank_db.dim_customer_lifecycle_zip
OK
_col0	_col1	_col2	_col3	_col4	_col5	_col6	_col7	_col8	_col9	_col10	_col11	_col12	_col13	_col14	_col15
Time taken: 4.754 seconds
Query ID = niit_20241218173831_18332949-54a9-45ff-9404-7c127e59d962
Total jobs = 3
Launching Job 1 out of 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Spark Job = 30008fb5-ec72-4196-8e12-3716e49de4e8
Running with YARN Application = application_1732546889003_0068
Kill Command = /opt/onlineedu/hadoop/bin/yarn application -kill application_1732546889003_0068

Query Hive on Spark job[2] stages: [5]

Status: Running (Hive on Spark job[2])
Job Progress Format
CurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount
2024-12-18 17:38:33,796	Stage-5_0: 0/1	
2024-12-18 17:38:34,800	Stage-5_0: 1/1 Finished	
Status: Finished successfully in 3.24 seconds
Launching Job 2 out of 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Spark Job = f68da4a7-adc1-488c-b0ec-67d04afc6a7e
2024-12-18 17:38:35,931	Stage-6_0: 1/1 Finished	
Status: Finished successfully in 1.00 seconds
Launching Job 3 out of 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Spark Job = 358425f7-54a2-4c4b-8e77-860fcb8fe3b0
Running with YARN Application = application_1732546889003_0068
Kill Command = /opt/onlineedu/hadoop/bin/yarn application -kill application_1732546889003_0068

Query Hive on Spark job[4] stages: [9, 7, 8]

Status: Running (Hive on Spark job[4])
Job Progress Format
CurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount
2024-12-18 17:38:38,027	Stage-7_0: 0/16	Stage-8_0: 0/16	Stage-9_0: 0/18	
2024-12-18 17:38:39,044	Stage-7_0: 2(+2)/16	Stage-8_0: 0/16	Stage-9_0: 0/18	
2024-12-18 17:38:40,059	Stage-7_0: 3(+3)/16	Stage-8_0: 0/16	Stage-9_0: 0/18	
2024-12-18 17:38:41,071	Stage-7_0: 5(+3)/16	Stage-8_0: 0/16	Stage-9_0: 0/18	
2024-12-18 17:38:42,106	Stage-7_0: 10(+3)/16	Stage-8_0: 0/16	Stage-9_0: 0/18	
2024-12-18 17:38:43,148	Stage-7_0: 13(+3)/16	Stage-8_0: 0/16	Stage-9_0: 0/18	
2024-12-18 17:38:44,156	Stage-7_0: 16/16 Finished	Stage-8_0: 2(+3)/16	Stage-9_0: 0/18	
2024-12-18 17:38:45,162	Stage-7_0: 16/16 Finished	Stage-8_0: 5(+3)/16	Stage-9_0: 0/18	
2024-12-18 17:38:46,247	Stage-7_0: 16/16 Finished	Stage-8_0: 11(+3)/16	Stage-9_0: 0/18	
2024-12-18 17:38:48,329	Stage-7_0: 16/16 Finished	Stage-8_0: 16/16 Finished	Stage-9_0: 2(+3)/18	
2024-12-18 17:38:49,334	Stage-7_0: 16/16 Finished	Stage-8_0: 16/16 Finished	Stage-9_0: 11(+3)/18	
2024-12-18 17:38:50,352	Stage-7_0: 16/16 Finished	Stage-8_0: 16/16 Finished	Stage-9_0: 18/18 Finished	
Status: Finished successfully in 14.35 seconds
Loading data to table default.merge_tmp_table
Loading data to table bank_db.dim_customer_lifecycle_zip
OK
target.row__id	target.customerid	target.surname	target.creditscore	target.geography	target.gender	target.age	target.tenure	target.balance	target.numofproducts	target.hascrcard	target.isactivemember	target.estimatedsalary	target.exited	target.effective_date	source.effective_date	_c16
Time taken: 19.744 seconds
Query ID = niit_20241218173850_09af1bec-f61a-4304-9ccd-62273c6c5a76
Total jobs = 1
Launching Job 1 out of 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Spark Job = b2c014ba-0675-42b9-bdc0-891ed99d1ba3
Running with YARN Application = application_1732546889003_0068
Kill Command = /opt/onlineedu/hadoop/bin/yarn application -kill application_1732546889003_0068

Query Hive on Spark job[5] stages: [10, 11]

Status: Running (Hive on Spark job[5])
Job Progress Format
CurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount
2024-12-18 17:38:51,976	Stage-10_0: 0/1	Stage-11_0: 0/16	
2024-12-18 17:38:52,981	Stage-10_0: 1/1 Finished	Stage-11_0: 2(+3)/16	
2024-12-18 17:38:53,985	Stage-10_0: 1/1 Finished	Stage-11_0: 5(+1)/16	
2024-12-18 17:38:54,988	Stage-10_0: 1/1 Finished	Stage-11_0: 7(+2)/16	
2024-12-18 17:38:55,992	Stage-10_0: 1/1 Finished	Stage-11_0: 9(+2)/16	
2024-12-18 17:38:56,997	Stage-10_0: 1/1 Finished	Stage-11_0: 13(+2)/16	
2024-12-18 17:38:58,000	Stage-10_0: 1/1 Finished	Stage-11_0: 16/16 Finished	
Status: Finished successfully in 7.03 seconds
Loading data to table bank_db.dim_customer_lifecycle_zip
OK
_col0	_col1	_col2	_col3	_col4	_col5	_col6	_col7	_col8	_col9	_col10	_col11	_col12	_col13	_col14	_col15
Time taken: 7.538 seconds
Query ID = niit_20241218173858_709eb13d-2553-42b1-b9ff-245c4fed208d
Total jobs = 3
Launching Job 1 out of 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Spark Job = fa3c322b-527a-46ff-8b84-0f40b5f09444
2024-12-18 17:38:59,516	Stage-12_0: 1/1 Finished	
Status: Finished successfully in 1.00 seconds
Launching Job 2 out of 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Spark Job = d17dd9cf-de3f-4284-8c7d-c933387795ad
2024-12-18 17:39:00,550	Stage-13_0: 1/1 Finished	
Status: Finished successfully in 1.00 seconds
Launching Job 3 out of 3
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Spark Job = 17edc2d4-0241-4694-9692-6345ccc6535b
Running with YARN Application = application_1732546889003_0068
Kill Command = /opt/onlineedu/hadoop/bin/yarn application -kill application_1732546889003_0068

Query Hive on Spark job[8] stages: [15, 16, 14]

Status: Running (Hive on Spark job[8])
Job Progress Format
CurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount
2024-12-18 17:39:02,623	Stage-14_0: 0/16	Stage-15_0: 0/16	Stage-16_0: 0/18	
2024-12-18 17:39:03,643	Stage-14_0: 1(+3)/16	Stage-15_0: 0/16	Stage-16_0: 0/18	
2024-12-18 17:39:04,664	Stage-14_0: 3(+3)/16	Stage-15_0: 0/16	Stage-16_0: 0/18	
2024-12-18 17:39:05,675	Stage-14_0: 7(+2)/16	Stage-15_0: 0/16	Stage-16_0: 0/18	
2024-12-18 17:39:06,689	Stage-14_0: 10(+3)/16	Stage-15_0: 0/16	Stage-16_0: 0/18	
2024-12-18 17:39:07,697	Stage-14_0: 15(+1)/16	Stage-15_0: 0(+1)/16	Stage-16_0: 0/18	
2024-12-18 17:39:08,706	Stage-14_0: 16/16 Finished	Stage-15_0: 3(+3)/16	Stage-16_0: 0/18	
2024-12-18 17:39:09,716	Stage-14_0: 16/16 Finished	Stage-15_0: 6(+3)/16	Stage-16_0: 0/18	
2024-12-18 17:39:10,726	Stage-14_0: 16/16 Finished	Stage-15_0: 13(+3)/16	Stage-16_0: 0/18	
2024-12-18 17:39:11,730	Stage-14_0: 16/16 Finished	Stage-15_0: 16/16 Finished	Stage-16_0: 1(+3)/18	
2024-12-18 17:39:12,733	Stage-14_0: 16/16 Finished	Stage-15_0: 16/16 Finished	Stage-16_0: 7(+3)/18	
2024-12-18 17:39:13,736	Stage-14_0: 16/16 Finished	Stage-15_0: 16/16 Finished	Stage-16_0: 18/18 Finished	
Status: Finished successfully in 13.15 seconds
Loading data to table default.merge_tmp_table
Loading data to table bank_db.dim_customer_lifecycle_zip
OK
target.row__id	target.customerid	target.surname	target.creditscore	target.geography	target.gender	target.age	target.tenure	target.balance	target.numofproducts	target.hascrcard	target.isactivemember	target.estimatedsalary	target.exited	target.effective_date	source.effective_date	_c16
Time taken: 15.832 seconds
Query ID = niit_20241218173914_b0a7f41e-dbd3-4554-993c-a2125b643518
Total jobs = 2
Launching Job 1 out of 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Spark Job = 6b5e8d51-934e-4247-b9fb-fe570eba9609
Running with YARN Application = application_1732546889003_0068
Kill Command = /opt/onlineedu/hadoop/bin/yarn application -kill application_1732546889003_0068

Query Hive on Spark job[9] stages: [17]

Status: Running (Hive on Spark job[9])
Job Progress Format
CurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount
2024-12-18 17:39:16,514	Stage-17_0: 0/1	
2024-12-18 17:39:17,518	Stage-17_0: 1/1 Finished	
Status: Finished successfully in 3.12 seconds
Launching Job 2 out of 2
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Spark Job = c2448b25-b4c8-4698-8477-e1af365fd498
Running with YARN Application = application_1732546889003_0068
Kill Command = /opt/onlineedu/hadoop/bin/yarn application -kill application_1732546889003_0068

Query Hive on Spark job[10] stages: [19, 18]

Status: Running (Hive on Spark job[10])
Job Progress Format
CurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount
2024-12-18 17:39:18,614	Stage-18_0: 0/16	Stage-19_0: 0/16	
2024-12-18 17:39:19,624	Stage-18_0: 3(+3)/16	Stage-19_0: 0/16	
2024-12-18 17:39:20,629	Stage-18_0: 7(+3)/16	Stage-19_0: 0/16	
2024-12-18 17:39:21,634	Stage-18_0: 12(+3)/16	Stage-19_0: 0/16	
2024-12-18 17:39:22,636	Stage-18_0: 16/16 Finished	Stage-19_0: 4(+3)/16	
2024-12-18 17:39:23,640	Stage-18_0: 16/16 Finished	Stage-19_0: 10(+3)/16	
2024-12-18 17:39:24,643	Stage-18_0: 16/16 Finished	Stage-19_0: 15(+1)/16	
2024-12-18 17:39:25,648	Stage-18_0: 16/16 Finished	Stage-19_0: 16/16 Finished	
Status: Finished successfully in 8.04 seconds
Loading data to table bank_db.dim_customer_lifecycle_zip
OK
customerid	surname	creditscore	geography	gender	age	tenure	balance	numofproducts	hascrcard	isactivemember	estimatedsalary	exited	effective_date	expiration_date	is_current
Time taken: 11.795 seconds
ODS 数据成功转换并加载到 DIM 层，日期: 2024-12-18
/usr/bin/which: no hbase in (/opt/onlineedu/rabbitmq/sbin:/opt/onlineedu/otp/erlang/bin:/opt/onlineedu/sqoop/bin:/opt/onlineedu/kafka/bin:/opt/onlineedu/hive/bin:/opt/onlineedu/spark/bin:/opt/onlineedu/scala/bin:/opt/onlineedu/hadoop/bin:/opt/onlineedu/hadoop/sbin:/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.312.b07-2.el8_5.x86_64//bin:/home/niit/.local/bin:/home/niit/bin:/opt/onlineedu/rabbitmq/sbin:/opt/onlineedu/otp/erlang/bin:/opt/onlineedu/sqoop/bin:/opt/onlineedu/kafka/bin:/opt/onlineedu/hive/bin:/opt/onlineedu/spark/bin:/opt/onlineedu/scala/bin:/opt/onlineedu/hadoop/bin:/opt/onlineedu/hadoop/sbin:/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.312.b07-2.el8_5.x86_64//bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin)
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/onlineedu/hive/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/onlineedu/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

Logging initialized using configuration in jar:file:/opt/onlineedu/hive/lib/hive-common-2.3.7.jar!/hive-log4j2.properties Async: true
Query ID = niit_20241218173930_8b4a5650-7f6a-4bdd-968a-91fbdc2f5ba0
Total jobs = 1
Launching Job 1 out of 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Spark Job = 2e85ae41-43a5-4472-a7ec-1ae920a81e7f
Running with YARN Application = application_1732546889003_0069
Kill Command = /opt/onlineedu/hadoop/bin/yarn application -kill application_1732546889003_0069

Query Hive on Spark job[0] stages: [0, 1]

Status: Running (Hive on Spark job[0])
Job Progress Format
CurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount
2024-12-18 17:39:51,194	Stage-0_0: 0/1	Stage-1_0: 0/1	
2024-12-18 17:39:54,310	Stage-0_0: 1/1 Finished	Stage-1_0: 0/1	
2024-12-18 17:39:55,316	Stage-0_0: 1/1 Finished	Stage-1_0: 1/1 Finished	
Status: Finished successfully in 6.29 seconds
Loading data to table bank_db.ads_customer_churn_rate
OK
_col0	_col1	_col2	_col3	_col4	_col5
Time taken: 25.815 seconds
Query ID = niit_20241218173955_3aa90ab4-4edb-483e-94c9-f81b9a4f7df6
Total jobs = 1
Launching Job 1 out of 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Spark Job = 6712c66d-0e4c-4657-a89d-2e9c4e4d80b4
Running with YARN Application = application_1732546889003_0069
Kill Command = /opt/onlineedu/hadoop/bin/yarn application -kill application_1732546889003_0069

Query Hive on Spark job[1] stages: [2, 3]

Status: Running (Hive on Spark job[1])
Job Progress Format
CurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount
2024-12-18 17:39:57,084	Stage-2_0: 0/1	Stage-3_0: 0/2	
2024-12-18 17:39:59,096	Stage-2_0: 1/1 Finished	Stage-3_0: 0/2	
2024-12-18 17:40:00,100	Stage-2_0: 1/1 Finished	Stage-3_0: 2/2 Finished	
Status: Finished successfully in 4.03 seconds
Loading data to table bank_db.ads_customer_balance_distribution
OK
balancerange	customercount
Time taken: 4.525 seconds
Query ID = niit_20241218174000_9378e808-cc8b-4b99-a70f-4ec2b9c27911
Total jobs = 1
Launching Job 1 out of 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Spark Job = ab5602ea-7462-430d-9229-7b260dd45396
2024-12-18 17:40:01,649	Stage-4_0: 1/1 Finished	Stage-5_0: 2/2 Finished	
Status: Finished successfully in 1.01 seconds
Loading data to table bank_db.ads_customer_feature_distribution
OK
gender	agegroup	customercount	activecustomercount	inactivecustomercount
Time taken: 2.039 seconds
Query ID = niit_20241218174002_ba7a4ab1-5caa-4b53-8b3f-b6ad70ef1def
Total jobs = 1
Launching Job 1 out of 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Spark Job = e020288b-e893-4c6d-af22-1184a0f37c13
Running with YARN Application = application_1732546889003_0069
Kill Command = /opt/onlineedu/hadoop/bin/yarn application -kill application_1732546889003_0069

Query Hive on Spark job[3] stages: [6, 7]

Status: Running (Hive on Spark job[3])
Job Progress Format
CurrentTime StageId_StageAttemptId: SucceededTasksCount(+RunningTasksCount-FailedTasksCount)/TotalTasksCount
2024-12-18 17:40:03,706	Stage-6_0: 0/1	Stage-7_0: 0/2	
2024-12-18 17:40:04,723	Stage-6_0: 1/1 Finished	Stage-7_0: 0(+1)/2	
2024-12-18 17:40:05,729	Stage-6_0: 1/1 Finished	Stage-7_0: 2/2 Finished	
Status: Finished successfully in 3.05 seconds
Loading data to table bank_db.ads_customer_value_segmentation
OK
_col0	_col1	_col2	_col3	_col4	_col5
Time taken: 3.681 seconds
DWD 数据成功转换并加载到 ADS 层，日期: 2024-12-18
清空 MySQL 表: ads_customer_churn_rate
mysql: [Warning] Using a password on the command line interface can be insecure.
执行 Sqoop 导出: ads_customer_churn_rate
Warning: /opt/onlineedu/sqoop/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /opt/onlineedu/sqoop/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /opt/onlineedu/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
24/12/18 17:40:07 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
24/12/18 17:40:07 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
24/12/18 17:40:07 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
24/12/18 17:40:07 INFO tool.CodeGenTool: Beginning code generation
24/12/18 17:40:07 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `ads_customer_churn_rate` AS t LIMIT 1
24/12/18 17:40:07 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `ads_customer_churn_rate` AS t LIMIT 1
24/12/18 17:40:07 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/onlineedu/hadoop
Note: /tmp/sqoop-niit/compile/277a4da7c794ca627e375f536a3aabff/ads_customer_churn_rate.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
24/12/18 17:40:08 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-niit/compile/277a4da7c794ca627e375f536a3aabff/ads_customer_churn_rate.jar
24/12/18 17:40:08 INFO mapreduce.ExportJobBase: Beginning export of ads_customer_churn_rate
24/12/18 17:40:08 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
24/12/18 17:40:09 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
24/12/18 17:40:09 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
24/12/18 17:40:09 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
24/12/18 17:40:09 INFO client.RMProxy: Connecting to ResourceManager at niit03/192.168.8.12:8032
24/12/18 17:40:10 INFO input.FileInputFormat: Total input files to process : 1
24/12/18 17:40:11 INFO input.FileInputFormat: Total input files to process : 1
24/12/18 17:40:11 INFO mapreduce.JobSubmitter: number of splits:4
24/12/18 17:40:11 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
24/12/18 17:40:11 INFO Configuration.deprecation: yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled
24/12/18 17:40:11 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1732546889003_0070
24/12/18 17:40:11 INFO impl.YarnClientImpl: Submitted application application_1732546889003_0070
24/12/18 17:40:11 INFO mapreduce.Job: The url to track the job: http://niit03:8088/proxy/application_1732546889003_0070/
24/12/18 17:40:11 INFO mapreduce.Job: Running job: job_1732546889003_0070
24/12/18 17:40:16 INFO mapreduce.Job: Job job_1732546889003_0070 running in uber mode : false
24/12/18 17:40:16 INFO mapreduce.Job:  map 0% reduce 0%
24/12/18 17:40:25 INFO mapreduce.Job:  map 25% reduce 0%
24/12/18 17:40:26 INFO mapreduce.Job:  map 100% reduce 0%
24/12/18 17:40:26 INFO mapreduce.Job: Job job_1732546889003_0070 completed successfully
24/12/18 17:40:26 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=832008
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=2730
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=19
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=4
		Data-local map tasks=4
		Total time spent by all maps in occupied slots (ms)=59098
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=29549
		Total vcore-milliseconds taken by all map tasks=29549
		Total megabyte-milliseconds taken by all map tasks=30258176
	Map-Reduce Framework
		Map input records=24
		Map output records=24
		Input split bytes=706
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=752
		CPU time spent (ms)=2430
		Physical memory (bytes) snapshot=811786240
		Virtual memory (bytes) snapshot=9469739008
		Total committed heap usage (bytes)=439877632
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
24/12/18 17:40:26 INFO mapreduce.ExportJobBase: Transferred 2.666 KB in 17.5324 seconds (155.7117 bytes/sec)
24/12/18 17:40:26 INFO mapreduce.ExportJobBase: Exported 24 records.
Sqoop 导出成功: ads_customer_churn_rate
删除 Sqoop 生成的 .java 文件
清空 MySQL 表: ads_customer_balance_distribution
mysql: [Warning] Using a password on the command line interface can be insecure.
执行 Sqoop 导出: ads_customer_balance_distribution
Warning: /opt/onlineedu/sqoop/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /opt/onlineedu/sqoop/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /opt/onlineedu/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
24/12/18 17:40:27 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
24/12/18 17:40:27 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
24/12/18 17:40:27 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
24/12/18 17:40:27 INFO tool.CodeGenTool: Beginning code generation
24/12/18 17:40:28 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `ads_customer_balance_distribution` AS t LIMIT 1
24/12/18 17:40:28 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `ads_customer_balance_distribution` AS t LIMIT 1
24/12/18 17:40:28 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/onlineedu/hadoop
Note: /tmp/sqoop-niit/compile/3e200c393c0a78ef2cfd671c9b466ae8/ads_customer_balance_distribution.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
24/12/18 17:40:29 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-niit/compile/3e200c393c0a78ef2cfd671c9b466ae8/ads_customer_balance_distribution.jar
24/12/18 17:40:29 INFO mapreduce.ExportJobBase: Beginning export of ads_customer_balance_distribution
24/12/18 17:40:29 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
24/12/18 17:40:29 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
24/12/18 17:40:29 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
24/12/18 17:40:29 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
24/12/18 17:40:29 INFO client.RMProxy: Connecting to ResourceManager at niit03/192.168.8.12:8032
24/12/18 17:40:30 INFO input.FileInputFormat: Total input files to process : 2
24/12/18 17:40:30 INFO input.FileInputFormat: Total input files to process : 2
24/12/18 17:40:31 INFO mapreduce.JobSubmitter: number of splits:3
24/12/18 17:40:31 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
24/12/18 17:40:31 INFO Configuration.deprecation: yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled
24/12/18 17:40:32 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1732546889003_0071
24/12/18 17:40:32 INFO impl.YarnClientImpl: Submitted application application_1732546889003_0071
24/12/18 17:40:32 INFO mapreduce.Job: The url to track the job: http://niit03:8088/proxy/application_1732546889003_0071/
24/12/18 17:40:32 INFO mapreduce.Job: Running job: job_1732546889003_0071
24/12/18 17:40:37 INFO mapreduce.Job: Job job_1732546889003_0071 running in uber mode : false
24/12/18 17:40:37 INFO mapreduce.Job:  map 0% reduce 0%
24/12/18 17:40:42 INFO mapreduce.Job:  map 33% reduce 0%
24/12/18 17:40:43 INFO mapreduce.Job:  map 100% reduce 0%
24/12/18 17:40:43 INFO mapreduce.Job: Job job_1732546889003_0071 completed successfully
24/12/18 17:40:43 INFO mapreduce.Job: Counters: 31
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=623982
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=770
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=18
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=3
		Data-local map tasks=2
		Rack-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=20522
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=10261
		Total vcore-milliseconds taken by all map tasks=10261
		Total megabyte-milliseconds taken by all map tasks=10507264
	Map-Reduce Framework
		Map input records=3
		Map output records=3
		Input split bytes=692
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=337
		CPU time spent (ms)=1730
		Physical memory (bytes) snapshot=608878592
		Virtual memory (bytes) snapshot=7105687552
		Total committed heap usage (bytes)=305135616
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
24/12/18 17:40:43 INFO mapreduce.ExportJobBase: Transferred 770 bytes in 14.1121 seconds (54.5633 bytes/sec)
24/12/18 17:40:43 INFO mapreduce.ExportJobBase: Exported 3 records.
Sqoop 导出成功: ads_customer_balance_distribution
删除 Sqoop 生成的 .java 文件
清空 MySQL 表: ads_customer_feature_distribution
mysql: [Warning] Using a password on the command line interface can be insecure.
执行 Sqoop 导出: ads_customer_feature_distribution
Warning: /opt/onlineedu/sqoop/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /opt/onlineedu/sqoop/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /opt/onlineedu/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
24/12/18 17:40:44 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
24/12/18 17:40:44 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
24/12/18 17:40:44 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
24/12/18 17:40:44 INFO tool.CodeGenTool: Beginning code generation
24/12/18 17:40:45 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `ads_customer_feature_distribution` AS t LIMIT 1
24/12/18 17:40:45 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `ads_customer_feature_distribution` AS t LIMIT 1
24/12/18 17:40:45 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/onlineedu/hadoop
Note: /tmp/sqoop-niit/compile/2878aeb685b4570f4fca15d9618e236d/ads_customer_feature_distribution.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
24/12/18 17:40:45 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-niit/compile/2878aeb685b4570f4fca15d9618e236d/ads_customer_feature_distribution.jar
24/12/18 17:40:45 INFO mapreduce.ExportJobBase: Beginning export of ads_customer_feature_distribution
24/12/18 17:40:46 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
24/12/18 17:40:47 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
24/12/18 17:40:47 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
24/12/18 17:40:47 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
24/12/18 17:40:47 INFO client.RMProxy: Connecting to ResourceManager at niit03/192.168.8.12:8032
24/12/18 17:40:49 INFO input.FileInputFormat: Total input files to process : 2
24/12/18 17:40:49 INFO input.FileInputFormat: Total input files to process : 2
24/12/18 17:40:49 INFO mapreduce.JobSubmitter: number of splits:4
24/12/18 17:40:49 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
24/12/18 17:40:49 INFO Configuration.deprecation: yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled
24/12/18 17:40:49 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1732546889003_0072
24/12/18 17:40:49 INFO impl.YarnClientImpl: Submitted application application_1732546889003_0072
24/12/18 17:40:49 INFO mapreduce.Job: The url to track the job: http://niit03:8088/proxy/application_1732546889003_0072/
24/12/18 17:40:49 INFO mapreduce.Job: Running job: job_1732546889003_0072
24/12/18 17:40:54 INFO mapreduce.Job: Job job_1732546889003_0072 running in uber mode : false
24/12/18 17:40:54 INFO mapreduce.Job:  map 0% reduce 0%
24/12/18 17:41:03 INFO mapreduce.Job:  map 25% reduce 0%
24/12/18 17:41:04 INFO mapreduce.Job:  map 100% reduce 0%
24/12/18 17:41:04 INFO mapreduce.Job: Job job_1732546889003_0072 completed successfully
24/12/18 17:41:05 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=832180
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1097
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=19
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=4
		Data-local map tasks=4
		Total time spent by all maps in occupied slots (ms)=58958
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=29479
		Total vcore-milliseconds taken by all map tasks=29479
		Total megabyte-milliseconds taken by all map tasks=30186496
	Map-Reduce Framework
		Map input records=8
		Map output records=8
		Input split bytes=756
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=760
		CPU time spent (ms)=3640
		Physical memory (bytes) snapshot=805396480
		Virtual memory (bytes) snapshot=9469267968
		Total committed heap usage (bytes)=389021696
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
24/12/18 17:41:05 INFO mapreduce.ExportJobBase: Transferred 1.0713 KB in 17.6431 seconds (62.1772 bytes/sec)
24/12/18 17:41:05 INFO mapreduce.ExportJobBase: Exported 8 records.
Sqoop 导出成功: ads_customer_feature_distribution
删除 Sqoop 生成的 .java 文件
清空 MySQL 表: ads_customer_value_segmentation
mysql: [Warning] Using a password on the command line interface can be insecure.
执行 Sqoop 导出: ads_customer_value_segmentation
Warning: /opt/onlineedu/sqoop/../hbase does not exist! HBase imports will fail.
Please set $HBASE_HOME to the root of your HBase installation.
Warning: /opt/onlineedu/sqoop/../hcatalog does not exist! HCatalog jobs will fail.
Please set $HCAT_HOME to the root of your HCatalog installation.
Warning: /opt/onlineedu/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
24/12/18 17:41:05 INFO sqoop.Sqoop: Running Sqoop version: 1.4.7
24/12/18 17:41:05 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
24/12/18 17:41:05 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
24/12/18 17:41:05 INFO tool.CodeGenTool: Beginning code generation
24/12/18 17:41:06 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `ads_customer_value_segmentation` AS t LIMIT 1
24/12/18 17:41:06 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `ads_customer_value_segmentation` AS t LIMIT 1
24/12/18 17:41:06 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/onlineedu/hadoop
Note: /tmp/sqoop-niit/compile/38fd2b5d4224bfdf7a4987083d581bf2/ads_customer_value_segmentation.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
24/12/18 17:41:07 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-niit/compile/38fd2b5d4224bfdf7a4987083d581bf2/ads_customer_value_segmentation.jar
24/12/18 17:41:07 INFO mapreduce.ExportJobBase: Beginning export of ads_customer_value_segmentation
24/12/18 17:41:07 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
24/12/18 17:41:07 INFO Configuration.deprecation: mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
24/12/18 17:41:07 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
24/12/18 17:41:07 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
24/12/18 17:41:07 INFO client.RMProxy: Connecting to ResourceManager at niit03/192.168.8.12:8032
24/12/18 17:41:09 INFO input.FileInputFormat: Total input files to process : 2
24/12/18 17:41:09 INFO input.FileInputFormat: Total input files to process : 2
24/12/18 17:41:10 INFO mapreduce.JobSubmitter: number of splits:3
24/12/18 17:41:10 INFO Configuration.deprecation: mapred.map.tasks.speculative.execution is deprecated. Instead, use mapreduce.map.speculative
24/12/18 17:41:10 INFO Configuration.deprecation: yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled
24/12/18 17:41:10 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1732546889003_0073
24/12/18 17:41:10 INFO impl.YarnClientImpl: Submitted application application_1732546889003_0073
24/12/18 17:41:10 INFO mapreduce.Job: The url to track the job: http://niit03:8088/proxy/application_1732546889003_0073/
24/12/18 17:41:10 INFO mapreduce.Job: Running job: job_1732546889003_0073
24/12/18 17:41:15 INFO mapreduce.Job: Job job_1732546889003_0073 running in uber mode : false
24/12/18 17:41:15 INFO mapreduce.Job:  map 0% reduce 0%
24/12/18 17:41:24 INFO mapreduce.Job:  map 100% reduce 0%
24/12/18 17:41:25 INFO mapreduce.Job: Job job_1732546889003_0073 completed successfully
24/12/18 17:41:25 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=624147
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=895
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=18
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Job Counters 
		Launched map tasks=3
		Data-local map tasks=3
		Total time spent by all maps in occupied slots (ms)=42152
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=21076
		Total vcore-milliseconds taken by all map tasks=21076
		Total megabyte-milliseconds taken by all map tasks=21581824
	Map-Reduce Framework
		Map input records=3
		Map output records=3
		Input split bytes=682
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=554
		CPU time spent (ms)=1910
		Physical memory (bytes) snapshot=626163712
		Virtual memory (bytes) snapshot=7134302208
		Total committed heap usage (bytes)=320864256
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=0
24/12/18 17:41:25 INFO mapreduce.ExportJobBase: Transferred 895 bytes in 17.5556 seconds (50.9808 bytes/sec)
24/12/18 17:41:25 INFO mapreduce.ExportJobBase: Exported 3 records.
Sqoop 导出成功: ads_customer_value_segmentation
删除 Sqoop 生成的 .java 文件
ADS 数据成功导出到 MySQL，所有操作完成。
自动化分析完成，日期: 2024-12-18
